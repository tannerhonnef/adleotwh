{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannerhonnef/adleotwh/blob/main/assignment4_twh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying Deep Learning to Earth Observation - Assignment 4**"
      ],
      "metadata": {
        "id": "MM6FkL2SjM6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "Work through the assignment 2 notebook, and use this notebook to provide your answers. Be aware that the code cell for `customDataset` in this template is with complete instruction. \n",
        "\n",
        "To submit the assignment, you will need to use GitHub and the existing private repository you already created called `adleoxyz` (xyz is your initials)\n",
        "\n",
        "Once you have completed the assignment:\n",
        "- Commit your notebook from colab to your private GitHub repo\n",
        "- The notebook should be named assignment4_xyz.ipynb, with xyz again replaced by your initials.\n",
        "- There are 50 points in this assignment, with an additional 5 extra credit. "
      ],
      "metadata": {
        "id": "63sSTy-BjbS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical questions**"
      ],
      "metadata": {
        "id": "mU95NKzqm0Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q1** \n",
        "a) \n",
        "\n",
        "The disadvantage of using a larger kernel size is the increased number of parameters associated with the model which will also require additional computational power.\n",
        "\n",
        "By making the network deeper by adding more convolutional layers, the model will become more complex and will be more prone to overfitting.\n",
        "\n",
        "By pooling or performing convolution with a stride greater than 1, it is likely that there will be a loss of data because the fine details may not be reflected after the operation had been performed.\n",
        "\n",
        "Dilated convolutions will lead to the same problems as pooling or convolutions in that the fine details will likely be lost by performing it. \n",
        "\n",
        "\n",
        "\n",
        "b)\n",
        "\n",
        "One solution to using dilated convolution layers is to carefully select the rate of the dilated convolution in order to keep as much of the fine detail in the imagery as possible."
      ],
      "metadata": {
        "id": "J-OwnOYNmudn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q2**\n",
        "a) \n",
        "\n",
        "ERF1 = 3+(3-1)x(1-1) = 3\n",
        "\n",
        "ERF2 = 3+(3+(3-1))x(1-1) = 3\n",
        "\n",
        "ERF3 = 3+(3+(3-1))x(1-1) = 3\n",
        "\n",
        "b)\n",
        "\n",
        "ERF1 = 3+(3-1)x(2-1) = 5\n",
        "\n",
        "ERF2 = 5+(3+(3-1))x(2-1) = 10\n",
        "\n",
        "ERF3 = 10+(3+(3-1))x(2-1) = 15\n",
        "\n",
        "c)\n",
        "\n",
        "ERF1 = 3+(3-1)x(2-1) = 5\n",
        "\n",
        "ERF2 = 5+(3+(3-1))x(4-1) = 14\n",
        "\n",
        "ERF3 = 14+(3+(3-1))x(8-1) = 31"
      ],
      "metadata": {
        "id": "Cajkm0yLm7nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q3**\n",
        "Parallel and dilated convolutions both increase the receptive field of the model while preserving the resolution of the model.  When arranged serially and the dilation rate is fixed among consecutive layers, resolution will be preserved and the receptive field will increase in a linear manner.  Parallel connections create a multi branch network with outputs at different spatial resolutions.  These outputs are fused together and are able to increase the receptive field and preserve spatial resolution."
      ],
      "metadata": {
        "id": "ZXKsK64_nFyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Continue with our pipeline implementation**"
      ],
      "metadata": {
        "id": "Vz2OmIrBlceE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oiRzE4uliyfk",
        "outputId": "c7a258d3-1eaf-4a22-ad66-a6d0f35edaaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "2L199Zd1mQmE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio\n",
        "import time\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "G8bnj2T6jZsM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-process the input dataset**"
      ],
      "metadata": {
        "id": "oJtRCLfXlb10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example code is provided below, which will allow you to get up and running with this assignment. However, you will learn best if you use the code you developed/modified from previous assignments to do the work, as you will start to see how it all fits together. "
      ],
      "metadata": {
        "id": "0sxDF0xfnjh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add code for input normalization or use the existing one\n",
        "\n",
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "BffGjf8pKDbz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the image augmentations functions of your choice here, or use the code below\n",
        "\n",
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either \n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "        \n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "        \n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "        \n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "        \n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle \n",
        "            (in degrees) respectively. If number of elements are bigger than 2, \n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "    \n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2], \n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2], \n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ],
      "metadata": {
        "id": "O2lQwmu7kWP_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For assignment 4, you are working with a dataset called \"PondDataset\" which consists pairs of already chipped image and labels of size: `256x256` and pixel values are already in the range of `[0, 1]`. \n",
        "\n",
        "**Structure:**\n",
        "\n",
        "![PondDataset](https://drive.google.com/uc?export=view&id=12Nuy_mVXSAQpnfmpOdycnLBxeEOajMSe)\n",
        "\n",
        "You can find the dataset in the shared drive, which is [here](https://drive.google.com/drive/folders/1hJKRa1tNQmglErELsIEk8hXEykJadmKh?usp=share_link). Please download the entire \"PondDataset\" folder and place it in a convenient locations in your own Google Drive \n",
        "\n",
        "Now you have to adapt your `ActiveLoadingDataset` from reading a \"csv file\", to walk through the folder structure and grab all the \"tiff\" files for \"image\" and \"label\" folders.\n",
        "\n"
      ],
      "metadata": {
        "id": "X7nb42nmkoDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Assignment Part 1\n",
        "\n"
      ],
      "metadata": {
        "id": "_Fj6IwWhnPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated above, you can adapt the code below, or you can use your own loader and adapt it as needed for this assignment. In this case, you need to modify the loader so that it can read chips from a directory, rather than just reading a CSV.  \n",
        "\n"
      ],
      "metadata": {
        "id": "CHBS1s04oFE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add custom dataset from previous assignment and modify to fit the requirements of assignment 4\n",
        "\n",
        "class ActiveLoadingDataset(Dataset):\n",
        "    def __init__(self, src_dir, dataset_name, usage, apply_normalization=False, \n",
        "                 transform=None, **kargs):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing \n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to  \n",
        "           be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "          \n",
        "        ###### Add your code here. ######\n",
        "        # hint: you need two lines of code or less: \n",
        "        #       2- make a list of filenames for the files with the \".tif\" \n",
        "        #          extension using os.walk and preferably list comprehension\n",
        "        #       3- sort the list of tif files.\n",
        "\n",
        "        img_dir = Path(src_dir) / self.usage / \"images\"\n",
        "        img_filenames = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
        "        img_filenames.sort()\n",
        "        lbl_dir = Path(src_dir) / self.usage / \"labels\"\n",
        "        lbl_filenames = [os.path.join(lbl_dir, f) for f in os.listdir(lbl_dir) if f.endswith('.tif')]\n",
        "        lbl_filenames.sort()\n",
        "        \n",
        "        ###### Add your code here. ######\n",
        "        # Do the same for labels.\n",
        "\n",
        "        self.img_chips = []\n",
        "        self.lbl_chips = []\n",
        "        \n",
        "        ##### Add your code here #####\n",
        "        # Hint: iterate through the list of string paths from the image and \n",
        "        # label lists, load the data, transpose image to have channels as the \n",
        "        # last dimension and add them to 'img_chips' and 'lbl_chips' containers.\n",
        "        # iterate through the images\n",
        "\n",
        "        \n",
        "\n",
        "        for img_file, lbl_file in zip(img_filenames, lbl_filenames):\n",
        "            with rasterio.open(img_file) as img:\n",
        "                img_data = np.transpose(img.read(), (1, 2, 0))\n",
        "            with rasterio.open(lbl_file) as lbl:\n",
        "                lbl_data = (lbl.read(1))\n",
        "            self.img_chips.append(img_data)\n",
        "            self.lbl_chips.append(lbl_data)\n",
        "\n",
        "        print('--------{} patches cropped--------'.format(len(self.img_chips)))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_chip = self.img_chips[index]\n",
        "        label_chip = self.lbl_chips[index]\n",
        "\n",
        "        # Change the code if you are using a different augmentation\n",
        "        if self.usage == \"train\" and self.transform:\n",
        "            \n",
        "            trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "            if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                image_chip, label_chip = flip_image_and_label(\n",
        "                    image_chip, label_chip, trans_flip\n",
        "                )\n",
        "            \n",
        "            if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                image_chip, label_chip = rotate_image_and_label(\n",
        "                    image_chip, label_chip, angle=[0,90]\n",
        "                )\n",
        "\n",
        "        # Convert numpy arrays to torch tensors.\n",
        "        # Image chips should be in 'CHW' order, if not transpose to correct \n",
        "        # order of dimensions.\n",
        "        ##### Add code here #####\n",
        "        image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1))).float()\n",
        "        label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip)).long()\n",
        "        # create the two tensors here for the output below, using the correct \n",
        "        # reshaping functions\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "n1SNASS_kD2u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading your data**"
      ],
      "metadata": {
        "id": "j2xVbJEqy2KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/PondDataset\"\n",
        "dataset_name = \"PondDataset\"\n",
        "\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]"
      ],
      "metadata": {
        "id": "OKYZ7Cb9y9e4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\", \n",
        "                                     apply_normalization=False, \n",
        "                                     transform=transform)"
      ],
      "metadata": {
        "id": "5dGykQgXzDl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46518ed4-962e-4274-932a-8cbdebafeb26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------359 patches cropped--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16, \n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "id": "5j3ElFJWzKw8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = ActiveLoadingDataset(src_dir, dataset_name, \n",
        "                                          usage=\"validation\", \n",
        "                                          apply_normalization=False)"
      ],
      "metadata": {
        "id": "WFJLUAfmzOlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7230459-39bf-453c-feaf-f5ec14cf2a83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------45 patches cropped--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(validation_dataset, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "id": "pUMP12gmzVkT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model architecture**"
      ],
      "metadata": {
        "id": "X4Zo8L59l6y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your reference I have added a complete Unet architecture."
      ],
      "metadata": {
        "id": "RCIIyjCM4B5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the Unet model you have designed from assignment 3 or use the existing one.\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    r\"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int)-- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        kernel_size (int) -- Size of convolution kernel.\n",
        "        stride (int) -- decides how jumpy kernel moves along the spatial \n",
        "            dimensions.\n",
        "        padding (int) -- how much the input should be padded on the borders with \n",
        "            zero.\n",
        "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
        "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float) -- dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation, \n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        if num_conv_layers > 1:\n",
        "            if drop_rate > 0:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride, \n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(drop_rate), \n",
        "                ] * (num_conv_layers - 1)\n",
        "            else:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), \n",
        "                ] * (num_conv_layers - 1)\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class UpconvBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Decoder layer decodes the features along the expansive path.\n",
        "    Args:\n",
        "        in_channels (int) -- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        upmode (str) -- Upsampling type. If \"fixed\" then a linear upsampling with scale factor\n",
        "                        of two will be applied using bi-linear as interpolation method.\n",
        "                        If deconv_1 is chosen then a non-overlapping transposed convolution will\n",
        "                        be applied to upsample the feature maps. If deconv_1 is chosen then an\n",
        "                        overlapping transposed convolution will be applied to upsample the feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
        "        super(UpconvBlock, self).__init__()\n",
        "\n",
        "        if upmode == \"fixed\":\n",
        "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True), ]\n",
        "            layers += [nn.BatchNorm2d(in_channels),\n",
        "                       nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False), ]\n",
        "\n",
        "        elif upmode == \"deconv_1\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, dilation=1), ]\n",
        "\n",
        "        elif upmode == \"deconv_2\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, dilation=1), ]\n",
        "\n",
        "        # Dense Upscaling Convolution\n",
        "        elif upmode == \"DUC\":\n",
        "            up_factor = 2\n",
        "            upsample_dim = (up_factor ** 2) * out_channels\n",
        "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3, padding=1),\n",
        "                      nn.BatchNorm2d(upsample_dim),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.PixelShuffle(up_factor), ]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.block(inputs)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=None, dropout_rate=0):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if not filter_config:\n",
        "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
        "\n",
        "        assert len(filter_config) == 6\n",
        "\n",
        "        # Contraction Path\n",
        "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 64x256x256\n",
        "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 128x128x128\n",
        "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 256x64x64\n",
        "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 512x32x32\n",
        "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 1024x16x16\n",
        "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 2048x8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Expansion Path\n",
        "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4], upmode=\"deconv_2\")  # 1024x16x16\n",
        "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3], upmode=\"deconv_2\")  # 512x32x32\n",
        "        self.conv2 = ConvBlock(filter_config[4], filter_config[3], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2], upmode=\"deconv_2\")  # 256x64x64\n",
        "        self.conv3 = ConvBlock(filter_config[3], filter_config[2], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1], upmode=\"deconv_2\")  # 128x128x128\n",
        "        self.conv4 = ConvBlock(filter_config[2], filter_config[1], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0], upmode=\"deconv_2\")  # 64x256x256\n",
        "        self.conv5 = ConvBlock(filter_config[1], filter_config[0], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1, stride=1, padding=0)  # classNumx256x256\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # set_trace()\n",
        "        e1 = self.encoder_1(inputs)  # batch size x 64 x 256 x 256\n",
        "        p1 = self.pool(e1)  # batch size x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.encoder_2(p1)  # batch size x 128 x 128 x 128\n",
        "        p2 = self.pool(e2)  # batch size x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.encoder_3(p2)  # batch size x 256 x 64 x 64\n",
        "        p3 = self.pool(e3)  # batch size x 256 x 32 x 32\n",
        "\n",
        "        e4 = self.encoder_4(p3)  # batch size x 512 x 32 x 32\n",
        "        p4 = self.pool(e4)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        e5 = self.encoder_5(p4)  # batch size x 1024 x 16 x 16\n",
        "        p5 = self.pool(e5)  # batch size x 1024 x 8 x 8\n",
        "\n",
        "        e6 = self.encoder_6(p5)  # batch size x 2048 x 8 x 8\n",
        "\n",
        "        d6 = self.decoder_1(e6)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        \n",
        "        skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 16 x 16\n",
        "\n",
        "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 32 x 32\n",
        "\n",
        "        d5_proper = self.conv2(skip2)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 64 x 64\n",
        "\n",
        "        d4_proper = self.conv3(skip3)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 128 x 128\n",
        "\n",
        "        d3_proper = self.conv4(skip4)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 256 x 256\n",
        "\n",
        "        d2_proper = self.conv5(skip5)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        d1 = self.classifier(d2_proper)  # batch size x classNum x 256 x 256\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "id": "syq2_DKrmBoL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing your model**"
      ],
      "metadata": {
        "id": "YWdmgmPIzeE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15"
      ],
      "metadata": {
        "id": "91DbioGYzluC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(n_classes, in_channels, filter_config, dropout_rate)"
      ],
      "metadata": {
        "id": "Qtiayi6AzoA-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Customized loss function**\n",
        "\n",
        "You will want to add two here, which you can copy from the main assignment notebook. "
      ],
      "metadata": {
        "id": "P4hMpDQ1ma0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add your choices of loss function here \n",
        "class BalancedCrossEntropyLoss(nn.Module):\n",
        "    '''\n",
        "    Balanced cross entropy loss by weighting of inverse class ratio\n",
        "    Params:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    '''\n",
        "\n",
        "    def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "        super(BalancedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        class_counts = torch.bincount(target.view(-1), minlength=predict.shape[1])\n",
        "        class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "        # set weight of ignore index to 0\n",
        "        if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "            class_weights[self.ignore_index] = 0\n",
        "\n",
        "        # normalize weights\n",
        "        class_weights /= torch.sum(class_weights)\n",
        "\n",
        "        # apply class weights to loss function\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.ignore_index, reduction=self.reduction)\n",
        "\n",
        "        return loss_fn(predict, target)"
      ],
      "metadata": {
        "id": "H8fe9coXmgOh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryDiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss of binary class\n",
        "        Params:\n",
        "            smooth (float): A float number to smooth loss, and avoid NaN error, default: 1\n",
        "            p (int): Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2. Used\n",
        "                     to control the sensitivity of the loss.\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, *]\n",
        "            target (torch.tensor): Target tensor of same shape with predict\n",
        "        Returns:\n",
        "            Loss tensor\n",
        "    '''\n",
        "    def __init__(self, smooth=1, p=1):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "\n",
        "        assert predict.shape == target.shape, \"predict & target shape do not match\"\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        num = 2 * (predict * target).sum() + self.smooth\n",
        "        den = (predict.pow(self.p) + target.pow(self.p)).sum() + self.smooth\n",
        "        loss = 1 - num / den\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss\n",
        "        Params:\n",
        "            weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "            ignore_index (int): Class index to ignore\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "            target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
        "            reduction (str): Reduction method.\n",
        "        Returns:\n",
        "            same as BinaryDiceLoss\n",
        "    '''\n",
        "    def __init__(self, weight=None, ignore_index=-100, smooth=1, p=1, reduction='sum'):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.dice = BinaryDiceLoss(smooth, p)\n",
        "        if weight is not None:\n",
        "            self.weight = weight.cuda()\n",
        "        else:\n",
        "            self.weight = None\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        #set_trace()\n",
        "        if predict.shape == target.shape:\n",
        "            pass\n",
        "        elif len(predict.shape) == 4:\n",
        "            target = F.one_hot(target, num_classes=predict.shape[1]).permute(0, 3, 1, 2).contiguous()\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        self.weight = torch.Tensor([1. / predict.shape[1]] * predict.shape[1]).cuda() if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        if self.ignore_index >= 0:\n",
        "            target = torch.where(target == self.ignore_index, torch.zeros_like(target), target)\n",
        "            predict = torch.where(target == self.ignore_index, torch.zeros_like(predict), predict)\n",
        "\n",
        "        total_loss = 0\n",
        "        for i in range(predict.shape[1]):\n",
        "            dice_loss = self.dice(predict[:, i], target[:, i])\n",
        "\n",
        "            assert self.weight.shape[0] == predict.shape[1], \\\n",
        "                    'Expect weight shape [{}], get[{}]'.format(predict.shape[1], self.weight.shape[0])\n",
        "            dice_loss *= self.weight[i]\n",
        "            total_loss += dice_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = total_loss / predict.shape[1]\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = total_loss\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "9EdBOlwLbpMU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding assignment part 2**: training the network\n",
        "\n"
      ],
      "metadata": {
        "id": "8M9vkOj7AT86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sections below you need to complete the three functions that you need to train and validate the network over a specified number of epochs  "
      ],
      "metadata": {
        "id": "eh13sfNJvDJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW To:**\n",
        "\n",
        "Properly use the \"criterion\" argument inside the \"train\" and \"validation\" functions:\n",
        "Pass the argument to the function as a string with `()` like: \"BalancedCrossEntropyLoss()\"\n",
        "\n",
        "Now as you try to use the argument inside both \"train\" and \"validation\" functions, use `eval()` like: \n",
        "\n",
        "`loss = eval(criterion)(tensor A, tensor B)`\n",
        "\n",
        "**Note:**\n",
        "\n",
        "`eval()` is a built-in Python function that allows you to evaluate a string expression as a Python code. It takes a string as an argument and evaluates the expression contained in it. The result of the evaluation is then returned."
      ],
      "metadata": {
        "id": "KWRrySwHBBT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to optimize over a batch of training images and labels\n",
        "def train(trainData, model, optimizer, criterion, device=True, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch \n",
        "                custom dataset (AquacultureData).\n",
        "            model -- Choice of segmentation model.\n",
        "            optimizer -- Chosen optimization algorithm to update model parameters.\n",
        "            criterion -- Chosen function to calculate loss over training samples.\n",
        "            device (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- ???????????????????????????\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_loss=[]\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        #Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "        img_chips = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "        # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "        optimizer.zero_grad()\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        pred = model(img_chips)\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        train_epoch_loss += loss.item()\n",
        "        # do the backward pass to calculate gradients with respect to the loss (1 line)\n",
        "        loss.backward()\n",
        "        # update model weights by invoking the proper method on 'optimizer'\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))"
      ],
      "metadata": {
        "id": "g-AEldksAikB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides training the network, it's important to evaluate its performance on a separate \"validation dataset\" to ensure that it's not overfitting to the training data. The validation process is similar to the training process, but the network is set to evaluation mode using `model.eval()` and the gradients are not computed."
      ],
      "metadata": {
        "id": "HvQWD_wTs6lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to iterate validation images and labels\n",
        "\n",
        "\n",
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset. This allows loss to calculate\n",
        "                at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "    eval_loss=[]\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        #Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "        image = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        pred = model(image)\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))\n"
      ],
      "metadata": {
        "id": "l2VxqLHiBDFi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the function to iterate over the desired number of epochs\n",
        "\n",
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder, initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "    \n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        gpu = True\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        gpu = False\n",
        "        model = model\n",
        "    \n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.98)\n",
        "\n",
        "    # Add your code here\n",
        "    for t in range(num_epochs):\n",
        "    # you need to loop through the epochs and perform the following:\n",
        "    # print the current epoch number out of the total epochs (e.g. \"epoch: 2/10\")(1 line)\n",
        "      print(f\"Epoch [{t+1}/{num_epochs}]\")\n",
        "    # start the timer (1 line)\n",
        "      start_time = datetime.now()\n",
        "    # do model fit on the training data for single epoch (1 line)\n",
        "      train(trainData, model, optimizer, criterion, device, train_loss=train_loss)\n",
        "    # do model validation on the validation dataset for one epoch (1 line)\n",
        "      validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "    # take a step to update the 'scheduler'. (1 line)\n",
        "      scheduler.step()\n",
        "    # Print the updated learning rate.\n",
        "      print(scheduler.get_last_lr())\n",
        "    # use \"add_scalars\" method with your writer to save the train and validation loss to graph\n",
        "      writer.add_scalar('Loss/train', np.random.random(), t)\n",
        "      writer.add_scalar('Loss/test', np.random.random(), t)\n",
        "      writer.add_scalar('Accuracy/train', np.random.random(), t)\n",
        "      writer.add_scalar('Accuracy/test', np.random.random(), t)\n",
        "\n",
        "    # using tensorboard package later. \n",
        "    \n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_time).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\".format(duration_format))"
      ],
      "metadata": {
        "id": "TQFVUtNtD2s_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrate the code\n",
        "Run the model training and validation for a specified number of epochs (e.g. 15), and then save the results. Train / validate twice, once using your first loss function, and again using your second loss function.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pG9Ni2kwKC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "initial_lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "criterion = \"BalancedCrossEntropyLoss()\"\n",
        "path =\"/content/gdrive/MyDrive/clarkFiles/geog215/assignment4/working/model1\"\n",
        "\n",
        "epoch_it =  epochIterater(train_loader, val_loader, model=model, criterion=criterion,\n",
        "                          WorkingFolder=path, initial_lr=initial_lr, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "wq1-0zAKyLQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e49aae-649e-4015-afa1-24aa3564336f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch [1/10]\n",
            "Training loss: 0.4184\n",
            "validation loss: 1.1558476832177904\n",
            "[0.001]\n",
            "Epoch [2/10]\n",
            "Training loss: 0.2979\n",
            "validation loss: 0.7719050778283013\n",
            "[0.001]\n",
            "Epoch [3/10]\n",
            "Training loss: 0.2444\n",
            "validation loss: 0.5679082920153936\n",
            "[0.00098]\n",
            "Epoch [4/10]\n",
            "Training loss: 0.2536\n",
            "validation loss: 0.4862559696038564\n",
            "[0.00098]\n",
            "Epoch [5/10]\n",
            "Training loss: 0.2288\n",
            "validation loss: 0.4129699945449829\n",
            "[0.00098]\n",
            "Epoch [6/10]\n",
            "Training loss: 0.2050\n",
            "validation loss: 0.46951961947811977\n",
            "[0.0009603999999999999]\n",
            "Epoch [7/10]\n",
            "Training loss: 0.1986\n",
            "validation loss: 0.519178647796313\n",
            "[0.0009603999999999999]\n",
            "Epoch [8/10]\n",
            "Training loss: 0.2236\n",
            "validation loss: 0.6124481298857265\n",
            "[0.0009603999999999999]\n",
            "Epoch [9/10]\n",
            "Training loss: 0.1889\n",
            "validation loss: 0.3771999110778173\n",
            "[0.0009411919999999999]\n",
            "Epoch [10/10]\n",
            "Training loss: 0.1757\n",
            "validation loss: 0.5428740438487795\n",
            "[0.0009411919999999999]\n",
            "--------------- Training finished in 0:00:10 ---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save model results 1 in a directory of choice in your gdrive\n",
        "path = \"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/working/model1/results\"\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "bLd8KNWID22z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train/validate 2\n",
        "num_epochs = 10\n",
        "initial_lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "criterion = \"DiceLoss()\"\n",
        "path =\"/content/gdrive/MyDrive/clarkFiles/geog215/assignment4/working/model2\"\n",
        "\n",
        "epoch_it =  epochIterater(train_loader, val_loader, model=model, criterion=criterion,\n",
        "                          WorkingFolder=path, initial_lr=initial_lr, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "P9zRPuuCyZlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ca4d0d-0635-4fb2-d18f-97e4b85a0190"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch [1/10]\n",
            "Training loss: 0.1431\n",
            "validation loss: 0.3039872381422255\n",
            "[0.001]\n",
            "Epoch [2/10]\n",
            "Training loss: 0.1221\n",
            "validation loss: 0.3294672012329102\n",
            "[0.001]\n",
            "Epoch [3/10]\n",
            "Training loss: 0.1057\n",
            "validation loss: 0.2806210047668881\n",
            "[0.00098]\n",
            "Epoch [4/10]\n",
            "Training loss: 0.1035\n",
            "validation loss: 0.35491414268811544\n",
            "[0.00098]\n",
            "Epoch [5/10]\n",
            "Training loss: 0.0993\n",
            "validation loss: 0.3570214006635878\n",
            "[0.00098]\n",
            "Epoch [6/10]\n",
            "Training loss: 0.1024\n",
            "validation loss: 0.2480929818418291\n",
            "[0.0009603999999999999]\n",
            "Epoch [7/10]\n",
            "Training loss: 0.1125\n",
            "validation loss: 0.28609017928441366\n",
            "[0.0009603999999999999]\n",
            "Epoch [8/10]\n",
            "Training loss: 0.1062\n",
            "validation loss: 0.2848147670427958\n",
            "[0.0009603999999999999]\n",
            "Epoch [9/10]\n",
            "Training loss: 0.0969\n",
            "validation loss: 0.2767730540699429\n",
            "[0.0009411919999999999]\n",
            "Epoch [10/10]\n",
            "Training loss: 0.0965\n",
            "validation loss: 0.2389099074734582\n",
            "[0.0009411919999999999]\n",
            "--------------- Training finished in 0:00:10 ---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save model results 1 in a directory of choice in your gdrive\n",
        "path = \"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/working/model2/results\"\n",
        "torch.save(model.state_dict(), path)\n"
      ],
      "metadata": {
        "id": "kyrDM0hqyeMz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation and accuracy metrics**"
      ],
      "metadata": {
        "id": "FaCaNlvonXp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "If you have disconnected from the Colab session or restarted the kernel, then before doing the evaluation on the validation dataset you must initialize your model once more and load the trained weights onto your model."
      ],
      "metadata": {
        "id": "Ey2Ep2LAoRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code for metric class\n",
        "\n",
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)"
      ],
      "metadata": {
        "id": "MLovzg7NngXX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Assignment Part 3\n",
        "\n",
        "Complete the code to undertake model evaluation below. Evaluate twice: once for each model trained with a different loss function.  "
      ],
      "metadata": {
        "id": "kux73Vkuy6bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_accuracy_evaluation(model, dataloader, num_classes, sv_dir):\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # add batch to evaluator\n",
        "            evaluator.add_batch(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    mean_accuracy = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_IoU = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_IoU = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    # save accuracies to csv\n",
        "    with open(sv_dir, mode='w') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['Pixel Accuracy', 'Mean Accuracy', 'Mean IoU', 'Frequency Weighted IoU'])\n",
        "        writer.writerow([pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU])\n",
        "\n",
        "    return pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU"
      ],
      "metadata": {
        "id": "QYZ4CIs5no54"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Demonstrate evaluation of model 1\n",
        "sv_dir = \"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/working/model1/accuracies.csv\"\n",
        "do_accuracy_evaluation(model, train_loader, num_classes=2, sv_dir=sv_dir)"
      ],
      "metadata": {
        "id": "W4WK7HeazNVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3708eda-d349-48fd-b548-55b4d27ed274"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.943795206819072, 0.8939267757098368, 0.8364519680264444, 0.895047474873476)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Demonstrate evaluation of model 2\n",
        "sv_dir = \"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/working/model2/accuracies.csv\"\n",
        "do_accuracy_evaluation(model, train_loader, num_classes=2, sv_dir=sv_dir)"
      ],
      "metadata": {
        "id": "7wKsdVoXzWdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927ca169-0910-491e-b6e8-59e894916d53"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.908692128811042, 0.7901484451529817, 0.7297397249170274, 0.8304577092395075)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing activation maps and learned kernels from intermidiate layers in the network**"
      ],
      "metadata": {
        "id": "_6tINf0Xo0vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the multispectral satellite image\n",
        "with rasterio.open(\"/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/PondDataset/train/images/LC08_L1TP_011060_20190426_20190426_01_RT_band_chip1.tif\") as dataset:\n",
        "    # Read the image data as a numpy array and reshape to HWC\n",
        "    image = dataset.read().transpose([1, 2, 0])\n",
        "\n",
        "# Normalize the image data to be between 0 and 1\n",
        "image = min_max_normalize_image(image)\n",
        "\n",
        "# Convert the image to a PyTorch tensor and add a batch dimension\n",
        "image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)"
      ],
      "metadata": {
        "id": "LS1rKkW3pEdi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model and load the pre-trained weights\n",
        "model = model\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/clarkFiles/dsci215/assignment4/working/model1/results'))\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "n4H0lHuj7sRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64e3f2f-4939-4629-9dc1-4dfa84f72cb6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet(\n",
            "  (encoder_1): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_2): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_3): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_4): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_5): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_6): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (decoder_1): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv1): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_2): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_3): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_4): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_5): UpconvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "OaE5oUzc7wST"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the activation map for a particular layer\n",
        "\n",
        "layer_index = 2  # Choose the index of the layer to visualize (0-indexed)\n",
        "layer_name = f'conv{layer_index + 1}'  # Layer name for displaying in the plot title\n",
        "activation = {}"
      ],
      "metadata": {
        "id": "HN8GqQIG7z5E"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "model.conv1.register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model(image_tensor)\n",
        "\n",
        "activation = activation.squeeze().numpy()\n",
        "\n",
        "plt.imshow(activation[layer_index])\n",
        "plt.title(f'Activation Map for {layer_name}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7y5mHSDm78xQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "2c8ab686-0416-4fc4-9153-e41f611d9b00"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3b953c8ca56e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0639b2b9f050>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch size x 64 x 256 x 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch size x 64 x 128 x 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0639b2b9f050>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ]
    }
  ]
}